{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Installing windtunnel package from GitHub...\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/se04ber/WTSoftwareUtilitiesShare.git\n",
      "  Cloning https://github.com/se04ber/WTSoftwareUtilitiesShare.git to /tmp/pip-req-build-ehge3g2b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/se04ber/WTSoftwareUtilitiesShare.git /tmp/pip-req-build-ehge3g2b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Resolved https://github.com/se04ber/WTSoftwareUtilitiesShare.git to commit 3d6e734ef74a76e1ec94206d273287693d3de7fc\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.8-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy\n",
      "  Downloading numpy-2.3.5-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m167.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m177.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.8.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (9.1 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m188.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy-stl\n",
      "  Downloading numpy_stl-3.2.0-py3-none-any.whl (20 kB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (355 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m355.2/355.2 kB\u001b[0m \u001b[31m299.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.61.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.0 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m179.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m228.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting packaging>=20.0\n",
      "  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m457.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Windtunnel Package Setup\n",
    "from deploy_config import install_windtunnel, verify_installation\n",
    "\n",
    "# Install and verify windtunnel package\n",
    "if not install_windtunnel():\n",
    "    print(\"‚ùå Installation failed. Please check your internet connection.\")\n",
    "else:\n",
    "    verify_installation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data source configuration\n",
    "USE_GITHUB_EXAMPLE_DATA = True #False #True  # Set to False to use local data\n",
    "#Else use your own Data in Folder Data/(your_data_folder)/measurement_prefix*, ...\n",
    "DATA_FOLDER_NAME = \"UBA Konvergenz\"          #name of folder inside Data/InputData/ f.e. Umrechnung zur Kontrolle / \n",
    "PARAMETER_FILE_NAME = \"ambient_conditions.csv\" #name of parameterFile inside ParameterFiles f.e. ambient_conditions_.UBA_GA.csv\n",
    "MEASUREMENT_PREFIX = \"your_measurement_prefix\" #Prefix of all time series files inside DATA_FOLDER_NAME f.e. UBA_GA_02_04_01_000_1_001\n",
    "\n",
    "# Data Setup and Configuration\n",
    "from deploy_config import setup_folder_structure, setup_github_data, setup_local_data\n",
    "# Setup folder structure and data\n",
    "base_dir, data_dir, input_dir, param_dir, results_dir = setup_folder_structure()\n",
    "if USE_GITHUB_EXAMPLE_DATA:\n",
    "    path_dir, path, csv_file, output_path, namelist = setup_github_data(input_dir, param_dir, results_dir)\n",
    "else:\n",
    "    # Local data configuration\n",
    "    DATA_FOLDER_NAME = DATA_FOLDER_NAME\n",
    "    PARAMETER_FILE_NAME = PARAMETER_FILE_NAME \n",
    "    MEASUREMENT_PREFIX = MEASUREMENT_PREFIX\n",
    "    path_dir, path, csv_file, output_path, namelist = setup_local_data(\n",
    "        input_dir, param_dir, results_dir, DATA_FOLDER_NAME, PARAMETER_FILE_NAME, MEASUREMENT_PREFIX\n",
    "    )\n",
    "print(f\"‚úÖ Setup complete! Data path: {path}, Output: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis Parameters\n",
    "# Measurement configuration\n",
    "parameters_PerFolder = False\n",
    "\n",
    "# Variables and Parameters set for all ts, if no ambient_conditions.csv file overgiven\n",
    "# If at the end calculate entdimensionalised or full scale transform quantities\n",
    "# Default: nd:entdimensionalise, ms:model scale, fs:full scale.    \n",
    "full_scale='ms'\n",
    "# Postprocessing before analysis\n",
    "applyPostprocessing=True\n",
    "averageInterval=60 # s  # Interval to downaverage raw time series to before analysis\n",
    "measurementFreq=0.005 # Time series frequency # For now only for static case implemented\n",
    "averagingColumns=[\"net_concentration\"] # Columns to average dow\n",
    "# Saving settings: (output_path for path)\n",
    "osType = \"Linux\" # Windows  # For Path\n",
    "outputName = None # Default: ts name\n",
    "\n",
    "saveTs=True    # Only save time series of concentration quantities to separat files\n",
    "saveAvg=True # Save average of ts of concentration quantities to separat files\n",
    "saveStats=True # Save averages, percentile95/5, peak2mean of ts of concentration quantities to separat files\n",
    "saveCombined=True # Save all averages and statistics for all files to one combined file\n",
    "combinedFileName=\"combined_file_nora.csv\"\n",
    "base_path=None # base_path = output_path + \"Files/Point_Data_stats/UBA_thesi/\" # Base path for getting files for combined files, if None\n",
    "\n",
    "saveAll=True # Sets saveTs, saveAvg and saveStats, saveCombined to True, saving ts, averages, statistics and combined file\n",
    "\n",
    "# Uncertainty calculation and saving\n",
    "calculateUncertainty=False #True # Calculate measurement uncertainties based on repeated measurements\n",
    "saveUncertainties=False #True # Add uncertainty columns to saved files\n",
    "saveConfigNames=True # Add config_name column to combined files\n",
    "split_factor=1.8 # Factor for splitting long time series in uncertainty calculation\n",
    "uncertainty_threshold=1e-4 # Threshold for relative deviation calculation\n",
    "verboseUncertainty=False #True # Print detailed logging for uncertainty calculation\n",
    "uncertaintyMetrics=None # Metrics to calculate uncertainties for: [\"Mean\", \"Median\", \"Peak2Mean\", \"P95\"] or None for all\n",
    "uncertaintyConcentrationTypes=None # Concentration types: [\"c_star\", \"net_concentration\", \"full_scale_concentration\"] or None for c_star only\n",
    "includeAbsoluteUncertainty=True # Include absolute uncertainty values (_uncertainty_abs)\n",
    "includePercentageUncertainty=True # Include percentage uncertainty values (_uncertainty_pct)\n",
    "\n",
    "# Column selection for combined files\n",
    "columnsToSave=None # None=default columns, or provide list e.g. [\"Avg_c_star [-]\", \"X_fs [m]\"]\n",
    "\n",
    "# Legacy uncertainty (for plotting)\n",
    "uncertainty_value=None # Manual uncertainty value for error bars\n",
    "uncertainty_representation=\"percentage\" # \"absoluteValue\"\n",
    "\n",
    "print(\"‚úÖ Analysis parameters configured!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example file/Default environment values if no csv_file found:\n",
    "\n",
    "#Source location  [mm]\n",
    "x_source=0\n",
    "y_source=0\n",
    "z_source=0\n",
    "#Source mass flow controller, calibration settings\n",
    "mass_flow_controller=0.300 #0.600#Stickstoffdurchflussregler #[l/h]*1/100 #'X'  #Controller(settings) used, just a name placeholder for orientation, not used yet\n",
    "#If calibration performed on a controller, corrects actual max. flow capacity of controller\n",
    "calibration_curve=1.0     #0.3     #0.3 oder 3\n",
    "calibration_factor=0 #1      #\n",
    "#Gas characteristics\n",
    "gas_name='C12'           #Just placeholder name variable for orientation, not used for anything\n",
    "gas_factor=0.5   #[-]    #!!! Needs to be calculate/specificate f.e. if gas changes \n",
    "mol_weight=29.0 #28.97 #Air [g/mol]\n",
    "#Measurement location [mm]\n",
    "x_measure=1020 #855.16\n",
    "y_measure= 0    #176.29\n",
    "z_measure= 5     #162\n",
    "#Surrounding conditions\n",
    "pressure=101426.04472        #1009.38  #[hPa] ->Pa\n",
    "temperature=23             #23.5  #[¬∞C]\n",
    "#Model to Reality scaling\n",
    "scale=400                     #250      #Model/Reality\n",
    "scaling_factor=0.5614882               #0.637       #USA1 to selected ref pos.?\n",
    "ref_length=1/400              #1/250           #Lref\n",
    "ref_height=100/400            #None            #Href\n",
    "#Full Scale Parameters\n",
    "full_scale_wtref=10             #6 #[m/s]         #Uref_fullscale\n",
    "full_scale_flow_rate=0.002     #Q_amb[kg/s]?   #0.5   #Qv_fullscale\n",
    "full_scale_temp=20             #[¬∞C]\n",
    "full_scale_pressure=101325     #[Pa]\n",
    "#Q_ambient[kg/s] ->  Q[m¬≥/s]=Q[kg/s]*R*T/(M*p)\n",
    "#Variable wdir for wind direction. To be implemented in future. ##\n",
    "#wdir=0\n",
    "#Variable axis_range. Reserved for future implementation of axis range specification, \n",
    "#analogously to puff mode\n",
    "#axis_range='auto'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import windtunnel as wt\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import importlib\n",
    "\n",
    "from windtunnel.concentration.CompareDatasets import *\n",
    "\n",
    "# Reload utils module to get latest changes\n",
    "if 'windtunnel.concentration.utils' in sys.modules:\n",
    "    importlib.reload(sys.modules['windtunnel.concentration.utils'])\n",
    "\n",
    "#Edited Nora: Check if ambient conditions file is even there, before trying out reading-in\n",
    "print(f\"CSV erwartet unter: {csv_file}\")\n",
    "if os.path.exists(csv_file):\n",
    "    print(\"‚úÖ CSV-Datei gefunden.\")\n",
    "else:\n",
    "    print(\"‚ùå CSV-Datei NICHT gefunden.\")\n",
    "    print(\"Folgende Dateien sind im Ordner vorhanden:\")\n",
    "    for f in os.listdir(path):\n",
    "        print(f)\n",
    "\n",
    "#Initialize uncertainty results\n",
    "uncertainty_results = {}\n",
    "        \n",
    "###### Initialise concentration ts dictionary of length namelist, as well as for full scale and entdimensionalised\n",
    "conc_ts = {}\n",
    "conc_ts.fromkeys(namelist)\n",
    "conc_ts_fs = conc_ts\n",
    "conc_ts_nd = conc_ts\n",
    "\n",
    "dict_conc_ts = conc_ts\n",
    "dict_conc_nd = conc_ts\n",
    "dict_conc_fs = conc_ts\n",
    "\n",
    "data_dict = {}\n",
    "data_dict.fromkeys(namelist)\n",
    "\n",
    "path = path + \"/\"\n",
    "#Read in ambient conditions for each folder or concentration ts from given csv file or for same conditions from manually\n",
    "parameters_PerFolder = parameters_PerFolder #False  #True=for each folder/namelist entry new column, False: for each ts one column entry\n",
    "\n",
    "for name in namelist:\n",
    "    files = wt.get_files(path, name)\n",
    "    print(f\"files: {files}\")\n",
    "\n",
    "    #Initilise Dictionary for each given name containing dimensions of nr of files ts#0-\n",
    "    conc_ts[name] = {}\n",
    "    conc_ts[name].fromkeys(files)\n",
    "    \n",
    "    if parameters_PerFolder==True:\n",
    "        #Read ambient conditions from csv file only for each folder\n",
    "        ambient_conditions = wt.PointConcentration.get_ambient_conditions(path=path, name=name, input_file=csv_file)\n",
    "        #print(ambient_conditions)\n",
    "        #Else read/use given default from cell above\n",
    "        if ambient_conditions is None:\n",
    "            []\n",
    "        #Read ambient conditions from csv file\n",
    "        else:\n",
    "            x_source, y_source, z_source, x_measure, y_measure, z_measure, pressure, temperature, calibration_curve, mass_flow_controller, calibration_factor, scaling_factor, scale, ref_length, \\\n",
    "            ref_height, gas_name, mol_weight, gas_factor, full_scale_wtref, full_scale_flow_rate, full_scale_temp, full_scale_pressure, config_name = wt.PointConcentration.read_ambient_conditions(\n",
    "                ambient_conditions, name)\n",
    "            \n",
    "        \n",
    "    for file in files:\n",
    "        if parameters_PerFolder == False:\n",
    "            #Read in ambient condition column for each ts\n",
    "            ambient_conditions = wt.PointConcentration.get_ambient_conditions(path=path, name=file, input_file=csv_file)\n",
    "            #Else read/use given default from cell above\n",
    "            if ambient_conditions is None:\n",
    "                []\n",
    "            #Read ambient conditions from csv file\n",
    "            else:\n",
    "                x_source, y_source, z_source, x_measure, y_measure, z_measure, pressure, temperature, calibration_curve, mass_flow_controller, calibration_factor, scaling_factor, scale, ref_length, \\\n",
    "                ref_height, gas_name, mol_weight, gas_factor, full_scale_wtref, full_scale_flow_rate, full_scale_temp, full_scale_pressure,  config_name= wt.PointConcentration.read_ambient_conditions(\n",
    "                ambient_conditions, file)\n",
    "\n",
    "        conc_ts[name][file] = wt.PointConcentration.from_file(path + file)\n",
    "    \n",
    "        conc_ts[name][file].ambient_conditions(x_source=x_source, y_source=y_source, z_source=z_source,\n",
    "                                               x_measure=x_measure, y_measure=y_measure, z_measure=z_measure,\n",
    "                                               pressure=pressure,\n",
    "                                               temperature=temperature,\n",
    "                                               calibration_curve=calibration_curve,\n",
    "                                               mass_flow_controller=mass_flow_controller,\n",
    "                                               calibration_factor=calibration_factor,\n",
    "                                               config_name=config_name)\n",
    "\n",
    "        #Set read-in scaling, gas and full scale information to internal class variables\n",
    "        print(\"Store information into PointConcentration class objects array\")\n",
    "        conc_ts[name][file].scaling_information(scaling_factor=scaling_factor, \n",
    "                                                scale=scale,\n",
    "                                                ref_length=ref_length, \n",
    "                                                ref_height=ref_height)\n",
    "        conc_ts[name][file].tracer_information(gas_name=gas_name,\n",
    "                                               mol_weight=mol_weight,\n",
    "                                               gas_factor=gas_factor)\n",
    "        conc_ts[name][file].full_scale_information(full_scale_wtref=full_scale_wtref,\n",
    "                                                   full_scale_flow_rate=full_scale_flow_rate,\n",
    "                                                   full_scale_temp=full_scale_temp,full_scale_pressure=full_scale_pressure)\n",
    "\n",
    "        #Calculate mass flow rate, net concentration and dimensionalise concentration\n",
    "        print(\"Do main calculations\")\n",
    "        conc_ts[name][file].convert_temperature()\n",
    "        conc_ts[name][file].calc_wtref_mean()\n",
    "        \n",
    "        conc_ts[name][file].calc_model_mass_flow_rate(usingMaxFlowRate=\"True\",applyCalibration=\"False\")\n",
    "        conc_ts[name][file].calc_net_concentration()\n",
    "\n",
    "        #conc_ts[name][file].clear_zeros()  #Remove values net_concentration =< 0 from dataset !noise\n",
    "        conc_ts[name][file].calc_c_star()\n",
    "\n",
    "        conc_ts[name][file].calc_full_scale_concentration() #Try\n",
    "\n",
    "        #Transforming/Outputting data in full-scale, model scale, and non-dimensional\n",
    "        print(\"Transform scale\")\n",
    "        if full_scale == 'ms':\n",
    "            dict_conc_ts = conc_ts\n",
    "            \n",
    "        elif full_scale == 'fs':\n",
    "            dict_conc_ts = conc_ts_fs\n",
    "            dict_conc_ts[name][file].to_full_scale()\n",
    "            \n",
    "        elif full_scale == 'nd':\n",
    "            dict_conc_ts = conc_ts_nd\n",
    "            dict_conc_ts[name][file].to_non_dimensional()\n",
    "        else:\n",
    "            print(\n",
    "                \"Error: invalid input for full_scale. Data can only be computed in model scale (full_scale='ms'), full scale (full_scale='fs'), or non-dimensionally (full_scale=nd).\")\n",
    "        #Apply Postprocessing if overgiven\n",
    "        \"\"\"\n",
    "        measurementFreq=measurementFreq #Time series frequency #For now only for static case implemented\n",
    "        averageInterval=averageInterval #60 #s\n",
    "        columns=averagingColumns #Columns to average down\n",
    "        #print(len(dict_conc_ts[name][file].net_concentration))\n",
    "        if(applyPostprocessing==True):\n",
    "                print(\"Apply postprocessing averaging: {averageInterval}s\")\n",
    "                dict_conc_ts[name][file].downAverage(averageInterval=averageInterval,measurementFreq=measurementFreq, columns=columns)\n",
    "                #dict_conc_ts[name][file].net_concentration\n",
    "        \n",
    "        #print(len(dict_conc_ts[name][file].net_concentration))\n",
    "        \"\"\"\n",
    "    \n",
    "    #Calculate measurement uncertainties before saving combined files\n",
    "    if calculateUncertainty and saveCombined:\n",
    "        from windtunnel.concentration.utils import calculate_uncertainties\n",
    "        if verboseUncertainty:\n",
    "            metrics_str = \", \".join(uncertaintyMetrics) if uncertaintyMetrics else \"all\"\n",
    "            conc_types_str = \", \".join(uncertaintyConcentrationTypes) if uncertaintyConcentrationTypes else \"c_star only\"\n",
    "            print(f\"\\nüìä Calculating measurement uncertainties for: {metrics_str} ({conc_types_str})\")\n",
    "        for name in namelist:\n",
    "            try:\n",
    "                uncertainty_results.update(calculate_uncertainties(\n",
    "                    conc_ts[name], \n",
    "                    split_factor=split_factor, \n",
    "                    uncertainty_threshold=uncertainty_threshold, \n",
    "                    verbose=verboseUncertainty,\n",
    "                    metrics_to_calculate=uncertaintyMetrics,\n",
    "                    concentration_types=uncertaintyConcentrationTypes,\n",
    "                    include_abs=includeAbsoluteUncertainty,\n",
    "                    include_pct=includePercentageUncertainty\n",
    "                ))\n",
    "            except TypeError as e:\n",
    "                print(f\"‚ö†Ô∏è Error with new parameters: {e}\")\n",
    "                print(\"‚ö†Ô∏è Falling back to basic parameters\")\n",
    "                uncertainty_results.update(calculate_uncertainties(conc_ts[name], split_factor, uncertainty_threshold))\n",
    "    \n",
    "    for name in namelist:\n",
    "        #Saving PointConcObject calculates new quantities(f.e. c*star) to files\n",
    "        if(saveAll):\n",
    "            saveTs=True\n",
    "            saveAvg=True\n",
    "            saveStats=True\n",
    "            saveCombined=True\n",
    "        if(saveCombined):\n",
    "            saveAvg=True\n",
    "            saveStats=True\n",
    "                    \n",
    "        if(saveTs==True or saveAvg==True or saveStats==True or saveCombined==True):\n",
    "            if osType==\"Windows\":\n",
    "                folder = 'Point_Data\\\\' + name[:name.find('.')] + '\\\\'\n",
    "                folder_avg = 'Point_Data_avg\\\\' + name[:name.find('.')] + '\\\\'\n",
    "                folder_stats = 'Point_Data_stats\\\\' + name[:name.find('.')] + '\\\\'\n",
    "            elif osType==\"Linux\":\n",
    "                 #print(\"gets here 2\")\n",
    "                 folder = 'Files/' + 'Point_Data/' + name[:name.find('.')] + '/'\n",
    "                 folder_avg = 'Files/' + 'Point_Data_avg/' + name[:name.find('.')] + '/'\n",
    "                 folder_stats = 'Files/' + 'Point_Data_stats/' + name[:name.find('.')] + '/'\n",
    "              \n",
    "            wt.check_directory(output_path + folder)\n",
    "            dict_conc_ts[next(iter(conc_ts))][next(iter(conc_ts[next(iter(conc_ts))]))].__check_sum = 8\n",
    "        #dict_conc_ts[name][file].__check_sum = 8\n",
    "\n",
    "        for file in files:\n",
    "            if(saveTs):\n",
    "                if full_scale == 'ms':\n",
    "                    dict_conc_ts[name][file].save2file_ms(file, out_dir=output_path + folder)\n",
    "                elif full_scale == 'fs':\n",
    "                    dict_conc_ts[name][file].save2file_fs(file, out_dir=output_path + folder)\n",
    "                elif full_scale == 'nd':\n",
    "                    dict_conc_ts[name][file].save2file_nd(file, out_dir=output_path + folder)\n",
    "                else:\n",
    "                    print(\n",
    "                        \"Error: invalid input for full_scale. Data can only be computed in model scale (full_scale='ms'), full scale (full_scale='fs'), or non-dimensionally (full_scale=nd).\")\n",
    "                    exit\n",
    "                print(\"Created ts files including (net_concentration, entimendionsliased and full scale concentration)\")\n",
    "            if(saveAvg):\n",
    "                 #Saving averages to files under folder Point_Data_stats/\n",
    "                #Averages of net_concentration,c_star and full_scale_concentration\n",
    "                 wt.check_directory(output_path + folder_avg)\n",
    "                 dict_conc_ts[name][file].save2file_avg(file, out_dir=output_path + folder_avg)\n",
    "                 print(f\"Created avg files under {output_path + folder_avg}\")\n",
    "            if(saveStats):\n",
    "                #Saving stats to files under folder Point_Data_avg/\n",
    "                #Stats Full ausgabe: saveAvg Quantities + Percentile 95, percentile 5, peak2Mean of net_concentration_c_star and full_Scale_conc\n",
    "                 wt.check_directory(output_path + folder_stats)\n",
    "                 dict_conc_ts[name][file].save2file_fullStats(file, out_dir=output_path + folder_stats)\n",
    "                 print(f\"Created stats files under {output_path + folder_stats}\")\n",
    "                \n",
    "        if(saveCombined):\n",
    "            #from windtunnel.concentration.utils import combine_to_csv2\n",
    "            stats=True\n",
    "            if(stats):\n",
    "                file_type=\"stats\"\n",
    "            \n",
    "            file_names = [\"_stats_\" + file for file in files]\n",
    "            if(base_path==None):\n",
    "                base_path = output_path + f\"Files/Point_Data_{file_type}/{name[0:-1]}/\"\n",
    "            \n",
    "            #combine_to_csv2(\n",
    "            #    file_names=file_names,\n",
    "            #    base_path=base_path,\n",
    "            #    file_type=file_type,\n",
    "            #    output_filename=output_path+combinedFileName,\n",
    "            #    conc_ts_dict=conc_ts[name],\n",
    "            #    uncertainty_results=uncertainty_results if calculateUncertainty else None,\n",
    "            #    save_config_names=saveConfigNames,\n",
    "            #    save_uncertainties=saveUncertainties and calculateUncertainty,\n",
    "            #    columns_to_save=columnsToSave\n",
    "            #)\n",
    "            from windtunnel.concentration.utils import *\n",
    "            combine_to_csv(file_names, base_path, file_type=file_type, output_filename=output_path+combinedFileName)\n",
    "            print(output_path+combinedFileName)\n",
    "            print(f\"Created combined file under{output_path + combinedFileName}\")\n",
    "            \n",
    "            # Mapping: Filename ‚Üí config_name\n",
    "            mapping = {file: conc_ts[name][file].config_name for name in namelist for file in conc_ts[name]}\n",
    "            df = pd.read_csv(\"Results/combined_file_nora.csv\")\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncertainty calculation is now integrated in Cell 2 and modularized in utils.calculate_uncertainties()\n",
    "#This cell is no longer needed - uncertainty calculation happens automatically before saveCombined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test read-in and net_conc and c*star output values, shape, nans and min/max\n",
    "#Test if net_concentration and c_star there\n",
    "\"\"\"if hasattr(conc_ts[name][file], \"c_star\"):\n",
    "    print(\"c_star vorhanden\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Keine c_star Daten gefunden.\")\n",
    "\n",
    "if hasattr(conc_ts[name][file], \"concentration\"):\n",
    "    if hasattr(conc_ts[name][file], \"net_concentration\"):\n",
    "        setattr(conc_ts[name][file], \"concentration\", conc_ts[name][file].net_concentration)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Kein concentration oder net_concentration gefunden.\")\"\"\"\n",
    "\n",
    " \n",
    "#Test numerical value range\n",
    "for name in namelist:\n",
    "    for file in files:\n",
    "        print(f\"\\n File: {file}\")\n",
    "        if hasattr(conc_ts[name][file], \"c_star\"):\n",
    "            print(\"c_star vorhanden\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Keine c_star Daten gefunden.\")\n",
    "\n",
    "        print(\"C_star shape:\", conc_ts[name][file].net_concentration.shape)\n",
    "        #print(conc_ts[name][file].net_concentration)\n",
    "        #plt.plot(conc_ts[namelist[0]][files[0]].c_star)\n",
    "        print(\"NaNs vorhanden:\", np.any(np.isnan(conc_ts[name][file].net_concentration)))\n",
    "        print(\"Min/Max:\", np.min(conc_ts[name][file].net_concentration), \"/\", np.max(conc_ts[name][file].net_concentration))\n",
    "        print(f\"Mean: {np.mean(conc_ts[name][file].net_concentration)}\")\n",
    "        print(f\"Std: {np.std(conc_ts[name][file].net_concentration)}\")\n",
    "        print(f\"Percentiles: {conc_ts[name][file].calc_percentiles(percentiles=[10, 90, 95], var='net_concentration')}\")    \n",
    "plt.show()\n",
    "#plt.violinplot([conc_ts[name][file].c_star for file in files])\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "#Read-in Data into separat array for later plotting (files/series can also be specifically choosen)\n",
    "dimensionless=True\n",
    "color=\"blue\"\n",
    "\n",
    "DataPointsConc = []\n",
    "#DataPointsConc = [ conc_ts[namelist[0]][\"MyFileofInterest1.txt\"], conc_ts[namelist[0]][\"MyFileofInterest2.txt\"]]\n",
    "for i in range(len(files)): #Just visualising all\n",
    "    data = conc_ts[namelist[0]][files[i]]\n",
    "    DataPointsConc.append(data)\n",
    "# Richtige Zeitserien laden\n",
    "labels=[f\"Dataset {i}\" for i in range(0,len(DataPointsConc))]\n",
    "plot_timeseries_with_stats(DataPointsConc,dimensionless=dimensionless,labels=labels,color=color)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#full_path = \"/Users/norakaiser/Masterthesis/WTSoftwareUtilitiesShare/test_meas/results/Point_Data_avg/UBA_thesis_test_01072025_001/_avg_UBA_thesis_test_01072025_001.ts#0\"\n",
    "full_path = \"/Users/norakaiser/Masterthesis/WTSoftwareUtilitiesShare/test_meas/results/Point_Data_avg/UBA_thesis_test_01072025_001/_avg_UBA_thesis_test_01072025_001.ts#0\"\n",
    "base_path = output_path + \"Files/Point_Data_avg/UBA_thesi/\"\n",
    "file_names = [\"_avg_\" + file for file in files]\n",
    "for file in file_names:\n",
    "    #metadata = {}\n",
    "    data = []\n",
    "    with open(full_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith('#'):\n",
    "                continue\n",
    "            if line.startswith('\"'):\n",
    "                # Kopfzeile mit Variablennamen\n",
    "                headers = line.replace('\"', '').split()\n",
    "                continue\n",
    "            # Wertezeile\n",
    "            values = list(map(float, line.split()))\n",
    "            data.append(values)\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=headers)\n",
    "    print(df.head())\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Convergence plot at different averaging intervals\n",
    "# SCATTER CONVERGENCE PLOT (plot_type=\"convergence\") (Scatter plot of all means at each averaging interval)\n",
    "# BANDWIDTH CONVERGENCE PLOT (plot_type=\"bandwidth\") (Max - Min of all means at each averaging interval)\n",
    "\n",
    "# Configuration\n",
    "plot_type = \"both\" #\"bandwidth\"  # Options: \"bandwidth\", \"convergence\", \"both\"\n",
    "dimensionless = False #True #False #True #False #True #True: Plot dimensionless c* on y-axsis instead of c_net[ppmV]\n",
    "percentage= True #False #True #False #True    #True: For Bandwidth plot: Plot Bandwidth percentage of mean\n",
    "visualizePercentage= True #False#True #False #True #True: For Bandwidth also add ticks with the actual values at the averaging_intervals points of the bandwidth/percentage\n",
    "save_to_csv = False  # Save bandwidth statistics to CSV\n",
    "csv_filename = output_path + \"convergence_bandwidth_stats.csv\"\n",
    "\n",
    "averaging_intervals = [60*0.01, 60*0.1, 60*0.5, 60*1.0, 60*2.0]  # in seconds\n",
    "time_freq = 0.010  # s #Manually implace frequence between each measurement point, if dynamic write time_freq= dynamic\n",
    "num_points_between = 5  # Number of points to add between each interval pair for more continuity(0 = use original intervals only)\n",
    "set_xlog = False #True #False #True #False #True #False #True  #Set x-axsis to log\n",
    "\n",
    "xLabel = None #\"Time Interval t[s]\"\n",
    "yLabel = None #\"Windspeed [m/s]\"\n",
    "DataColor = \"navy\"\n",
    "Labels = None\n",
    "xAchse = None\n",
    "yAchse = None\n",
    "error_values = 0.1\n",
    "marker = 'o'\n",
    "\n",
    "#For generate extended intervals/points if requested\n",
    "if num_points_between > 0:\n",
    "    extended_intervals = []\n",
    "    for i in range(len(averaging_intervals) - 1):\n",
    "        start = averaging_intervals[i]\n",
    "        end = averaging_intervals[i + 1]\n",
    "        #points = np.logspace(np.log10(start), np.log10(end), num_points_between + 2)\n",
    "        points = np.linspace(start, end, num_points_between + 2)\n",
    "        extended_intervals.extend(points[:-1])\n",
    "    extended_intervals.append(averaging_intervals[-1])\n",
    "    averaging_intervals_use_d = extended_intervals\n",
    "else:\n",
    "    averaging_intervals_use_d = averaging_intervals\n",
    "\n",
    "\n",
    "# BANDWIDTH CONVERGENCE PLOT \n",
    "if plot_type in [\"bandwidth\", \"both\"]:\n",
    "    bandwidths_all = []\n",
    "    ts_v_avg_all = []\n",
    "    dataset_labels = []\n",
    "    ts_v_avg_orig_all = [] if save_to_csv else None\n",
    "    \n",
    "    for name in namelist:\n",
    "        for file in files:\n",
    "            \n",
    "            if dimensionless:\n",
    "                #print(conc_ts[name][file].wtref_mean.shape)\n",
    "                conversion_factor = (conc_ts[name][file].wtref_mean / conc_ts[name][file].ref_length) #* conc_ts[name][file].time\n",
    "                print(conversion_factor)\n",
    "                #print(f\"A: {averaging_intervals_use_d}\")\n",
    "                averaging_intervals_use_axis = [interval * conversion_factor for interval in averaging_intervals_use_d]\n",
    "                averaging_intervals_list.append(averaging_intervals_use_axis)\n",
    "                #print(f\"B: {averaging_intervals_use}\")\n",
    "                x_axis_label = \"Averaging Intervals [t*]\"\n",
    "            else:\n",
    "                x_axis_label = \"Averaging Intervals t[s]\"\n",
    "            \n",
    "           \n",
    "            ts_v_avg = conc_ts[name][file].get_averagedData(name, file, time_freq, averaging_intervals_use_d, dimensionless)\n",
    "            #else:\n",
    "            #        ts_v_avg = ts_v_avg_orig  # Reuse original data\n",
    "            #else:\n",
    "            #    # Calculate for original intervals (needed for CSV and if no interpolation)\n",
    "            ts_v_avg_orig = conc_ts[name][file].get_averagedData(name, file, time_freq, averaging_intervals, dimensionless)\n",
    "            if save_to_csv:\n",
    "                ts_v_avg_orig_all.append(ts_v_avg_orig)\n",
    "\n",
    "            \n",
    "            ts_v_avg_all.append(ts_v_avg)\n",
    "            dataset_labels.append(f\"{name} - {file}\")\n",
    "            if percentage:\n",
    "                #print(\"gets here\")\n",
    "                bandwidths_all.append([(np.ptp(avg)/np.mean(avg) * 100) if np.mean(avg) != 0 else 0 for avg in ts_v_avg])\n",
    "                y_axis_label = \"Bandwidth [% of mean]\" \n",
    "            else:\n",
    "                bandwidths_all.append([np.ptp(avg) for avg in ts_v_avg])\n",
    "                if(dimensionless):\n",
    "                    y_axis_label = \"Bandwidth [ppmV]\"\n",
    "                else:\n",
    "                    y_axis_label = \"Bandwidth [-]\"\n",
    "    \n",
    "    n_datasets = len(bandwidths_all)\n",
    "    fig1, axes1 = plt.subplots(n_datasets, 1, figsize=(10, 4*n_datasets), sharex=False)\n",
    "    if n_datasets == 1:\n",
    "        axes1 = [axes1]\n",
    "    for i, (bandwidth, ts_v_avg, ax, label) in enumerate(zip(bandwidths_all, ts_v_avg_all, axes1, dataset_labels)):\n",
    "        print(f\"Dataset {i}: {bandwidth}\")\n",
    "        if dimensionless:\n",
    "            averaging_intervals_use = averaging_intervals_list[i]\n",
    "        else:\n",
    "            averaging_intervals_use = averaging_intervals_use_d\n",
    "            \n",
    "        ax.plot(averaging_intervals_use, bandwidth, \"o\",c=DataColor, label=label if Labels is None else Labels[i])\n",
    "\n",
    "        ax.fill_between(averaging_intervals_use, np.zeros_like(bandwidth), bandwidth, \n",
    "                       color='grey', alpha=0.3, zorder=1)\n",
    "        ax.set_xlabel(x_axis_label if xLabel is None else xLabel)\n",
    "        ax.set_ylabel(y_axis_label if yLabel is None else yLabel)\n",
    "        if xAchse != None: \n",
    "            ax.set_xlim(xAchse)\n",
    "        if yAchse != None: \n",
    "            ax.set_ylim(yAchse)\n",
    "        ax.set_title(f'Bandwidth Convergence: {label}')\n",
    "        if set_xlog:\n",
    "            ax.set_xscale('log')\n",
    "        if visualizePercentage:\n",
    "            averaging_intervals_use = averaging_intervals_use_d\n",
    "            for orig_interval in averaging_intervals:\n",
    "                if orig_interval in averaging_intervals_use:\n",
    "                    interval_idx = averaging_intervals_use.index(orig_interval)\n",
    "                    avg_data = ts_v_avg[interval_idx]\n",
    "                    mean_val = np.mean(avg_data)\n",
    "                    pct = (np.ptp(avg_data) / mean_val * 100) if mean_val != 0 else 0\n",
    "                    if(dimensionless == True):\n",
    "                        orig_interval = orig_interval * conversion_factor\n",
    "                    y_pos = bandwidth[interval_idx]\n",
    "                    ax.text(orig_interval, y_pos, f'{pct:.1f}%', \n",
    "                            fontsize=8, ha='center', va='bottom', color='darkred')\n",
    "        #ax.legend()\n",
    "        ax.grid(True, which='both', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# INTERVAL CONVERGENCE PLOT\n",
    "if plot_type in [\"convergence\", \"both\"]:\n",
    "    means_all = []\n",
    "    dataset_labels = []\n",
    "    for name in namelist:\n",
    "        for file in files:\n",
    "            if dimensionless:\n",
    "                #print(conc_ts[name][file].wtref_mean.shape)\n",
    "                conversion_factor = (conc_ts[name][file].wtref_mean / conc_ts[name][file].ref_length) #* conc_ts[name][file].time\n",
    "                averaging_intervals_use_axis = [conversion_factor * interval for interval in averaging_intervals_use_d]\n",
    "                averaging_intervals_list.append(averaging_intervals_use_axis)\n",
    "                x_axis_label = \"Averaging Intervals t[-]\"\n",
    "                y_axis_label = \"Mean concentration c[-]\"\n",
    "            else:\n",
    "                x_axis_label = \"Averaging Intervals t[s]\"\n",
    "                y_axis_label = \"Mean concentration c[ppmV]\"\n",
    "                \n",
    "            ts_v_avg = conc_ts[name][file].get_averagedData(name, file, time_freq, averaging_intervals_use_d,dimensionless)\n",
    "            means_all.append(ts_v_avg)\n",
    "            dataset_labels.append(f\"{name} - {file}\")\n",
    "            \n",
    "    \n",
    "    n_datasets = len(means_all) \n",
    "    fig2, axes = plt.subplots(n_datasets, 1, figsize=(10, 4*n_datasets), sharex=False)\n",
    "    if n_datasets == 1: # Handle single dataset case\n",
    "        axes = [axes]\n",
    "    for i, (means, ax, label) in enumerate(zip(means_all, axes, dataset_labels)):\n",
    "        # Collect min and max values for each interval for grey background coloring\n",
    "        min_vals = []\n",
    "        max_vals = []\n",
    "        intervals_with_data = []\n",
    "        if dimensionless:\n",
    "            averaging_intervals_use = averaging_intervals_list[i]\n",
    "          \n",
    "            \n",
    "        for interval, avg_data in zip(averaging_intervals_use, means):\n",
    "            if len(avg_data) > 0:\n",
    "                min_vals.append(np.min(avg_data))\n",
    "                max_vals.append(np.max(avg_data))\n",
    "                intervals_with_data.append(interval)\n",
    "        if len(intervals_with_data) > 0: #Also plot connected shaded region\n",
    "            ax.fill_between(intervals_with_data, min_vals, max_vals, \n",
    "                           color='grey', alpha=0.3, zorder=1)\n",
    "        #Satter points on top\n",
    "          \n",
    "        for interval, avg_data in zip(averaging_intervals_use, means):\n",
    "            ax.scatter(np.ones(len(avg_data)) * interval,\n",
    "                      avg_data, \n",
    "                      s=15, c=\"navy\", marker='o', alpha=0.6, zorder=2)\n",
    "        #ax.set_ylabel(yLabel)\n",
    "        ax.set_xlabel(x_axis_label if xLabel is None else xLabel)\n",
    "        ax.set_ylabel(y_axis_label if yLabel is None else yLabel)\n",
    "        if xAchse != None: \n",
    "            ax.set_xlim(xAchse)\n",
    "        if xAchse != None: \n",
    "            ax.set_ylim(yAchse)      \n",
    "        ax.grid(True, which='both', alpha=0.3)\n",
    "        if set_xlog:\n",
    "            ax.set_xscale('log')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Save bandwidth statistics to CSV (only original intervals, not interpolated)\n",
    "if save_to_csv and plot_type in [\"bandwidth\", \"both\"]:\n",
    "    csv_data = []\n",
    "    for idx, (name, file) in enumerate([(n, f) for n in namelist for f in files]):\n",
    "        row = {\"File\": file}\n",
    "        for interval, avg_data in zip(averaging_intervals, ts_v_avg_orig_all[idx]):\n",
    "            mean_val = np.mean(avg_data) if len(avg_data) > 0 else 0\n",
    "            min_val = np.min(avg_data) if len(avg_data) > 0 else 0\n",
    "            max_val = np.max(avg_data) if len(avg_data) > 0 else 0\n",
    "            bandwidth = np.ptp(avg_data) if len(avg_data) > 0 else 0\n",
    "            pct = (bandwidth / mean_val * 100) if mean_val != 0 else 0\n",
    "            row[f\"{interval:.3f}s_Min\"] = min_val\n",
    "            row[f\"{interval:.3f}s_Max\"] = max_val\n",
    "            row[f\"{interval:.3f}s_Bandwidth\"] = bandwidth\n",
    "            row[f\"{interval:.3f}s_Mean\"] = mean_val\n",
    "            row[f\"{interval:.3f}s_Percentage\"] = pct\n",
    "        csv_data.append(row)\n",
    "    df_csv = pd.DataFrame(csv_data)\n",
    "    csv_filename = csv_filename\n",
    "    df_csv.to_csv(csv_filename, index=False)\n",
    "    print(f\"‚úÖ Saved bandwidth statistics to {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4x1 Table means, std., peak2mean,percentile \n",
    "xLabels = \"Datasets\"\n",
    "yLabels = ['Mean  [ppmV]', '95th Percentile  [ppmV]', 'Std Dev.  [ppmV]','Peak/Mean  [-]'] #None #\"Concentration\"\n",
    "Labels = None\n",
    "dimensionless = False\n",
    "xAchse = None#ambient_conditions_.UBA_GA.csv\n",
    "yAchse = None\n",
    "markers = ['o', 's', '^', 'd']\n",
    "colors = [\"green\",\"blue\",\"red\",\"purple\"]\n",
    "\n",
    "error_values = 0.5 #[error_mean,error_95,error_std,error_peak/mean]\n",
    "percentileValue = 95 #percentil for percentilCalculation 95->95%\n",
    "\n",
    "# Calculate stats\n",
    "x = range(len(DataPointsConc))\n",
    "DataPointsConc2 = [DataPoints.net_concentration for DataPoints in DataPointsConc]\n",
    "stats = [[np.mean(d),  np.percentile(d, percentileValue), np.std(d),np.max(d)/np.mean(d)] for d in DataPointsConc2]\n",
    "stats = list(zip(*stats))\n",
    "errors = error_values if isinstance(error_values, list) else [error_values] * len(DataPointsConc)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(8, 10))\n",
    "axes_flat = axes.flatten()  # Convert 2x2 array to 1D array\n",
    "\n",
    "for i, ax in enumerate(axes_flat):\n",
    "    ax.errorbar(x, stats[i], yerr=errors, fmt=markers[i], capsize=3, color=colors[i])\n",
    "    if yAchse: \n",
    "        ax.set_ylim(yAchse)\n",
    "    if xAchse: \n",
    "        ax.set_xlim(xAchse)\n",
    "    if i >= 2:  # Bottom row subplots\n",
    "        ax.set_xlabel(xLabels)\n",
    "    ax.set_ylabel(yLabels[i])\n",
    "    ax.grid(True)   \n",
    "    ax.set_title(data.config_name)\n",
    "    # ax.set_xticks(x)\n",
    "    # ax.set_xticklabels([f'Dataset {j+1}' for j in x])\n",
    "\n",
    "plt.tight_layout()\n",
    "if Labels != None:\n",
    "    plt.legend(Labels)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xLabel=\"Datasets\"\n",
    "yLabel=\"Mean concentration[ppmV]\" \n",
    "dimensionless=\"False\"\n",
    "xAchse = None \n",
    "yAchse=None #(72,79) #None \n",
    "error_values=0.5 #[0.5,0.2,0.1] #For error values overgive one number which is cast to all values, or an array if specify different errors for each measurements\n",
    "errorType=\"absolute\"\n",
    "test = create_means(DataPointsConc,error_values,dimensionless=dimensionless,labels=None,xLabel=xLabel,yLabel=yLabel,xAchse=xAchse,yAchse=yAchse)\n",
    "#plt.savefig(\"Mean_comparison.png\",test) #To save image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xLabel=\"Concentration[-]\"\n",
    "yLabel=\"Density\"\n",
    "dimensionless=\"True\"\n",
    "create_pdf(DataPointsConc,dimensionless=\"True\",labels=None,xLabel=xLabel,yLabel=yLabel,xAchse=None,yAchse=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xLabel=\"Datasets\"\n",
    "yLabel=\"Concentration [-]\"\n",
    "dimensionless=\"True\"\n",
    "create_violinplot(DataPointsConc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xLabel=\"Datasets\"\n",
    "yLabel=\"Concentration [-]\"\n",
    "dimensionless=\"True\"\n",
    "create_boxplot(DataPointsConc,dimensionless=\"True\",labels=None,xLabel=xLabel,yLabel=yLabel,xAchse=None,yAchse=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xLabel=\"Datasets\"\n",
    "yLabel=\"Concentration[ppmV]\"\n",
    "xLabel=\"Datasets\"\n",
    "yLabel=\"Concentration[ppmV]\"\n",
    "create_histogram(DataPointsConc,dimensionless=\"False\",labels=None,xLabel=xLabel,yLabel=yLabel,xAchse=None,yAchse=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yLabel=None\n",
    "xLabel=\"Concentration[-]\"\n",
    "dimensionless=\"False\"\n",
    "create_cdf(DataPointsConc,dimensionless=dimensionless,labels=None,xLabel=xLabel,yLabel=yLabel,xAchse=None,yAchse=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = powerDensityPlot(DataPointsConc,dimensionless=\"False\",plot=True,labels=None,xLabel=None,yLabel=None,xAchse=None,yAchse=None)\n",
    "plt.savefig(\"test.png\",test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from windtunnel.concentration.CompareDatasets import compare_point_concentrations_3\n",
    "#Give overview/comparison for the data, one plot including of of the plots choosen in the list \"functionsForOverview\n",
    "#Means, Pdf, Cdf, PowerDensity: Number of ts does not matter that much\n",
    "#Scatterplot: use only 2 ts\n",
    "#Histogram, Boxplot: would also recommend not to many, because of overlapping of the histograms for comparison, space for the boxplots..\n",
    "\n",
    "#functionsForOverview = [\"all\"] #defaul -> all of the available plots\n",
    "#all_plot_types = [\n",
    "#        \"Histogram\", \"Pdf\", \"Cdf\", \"Means\", \"BoxPlot\", \n",
    "#        \"QuantilPlot\", \"ScatterPlot\", \"ResidualPlot\", \"Autocorrelation\"\n",
    "#    ]\n",
    "    \n",
    "functionsForOverview = [\n",
    "    \"Histogram\",\n",
    "    \"BoxPlot\"\n",
    "    #\"\",<\n",
    "    \"Pdf\",\n",
    "    \"Cdf\",\n",
    "    \"Means\",\n",
    "    \"PowerDensity\"\n",
    "        ]\n",
    "\n",
    "#Choose which concentration time series/PointObjekts to show in the overview/comparison plot\n",
    "DataPointsConc = [\n",
    "    conc_ts[namelist[0]][files[0]],#\n",
    "    conc_ts[namelist[0]][files[1]],\n",
    "    #conc_ts[namelist[0]][files[2]],\n",
    "    #conc_ts[namelist[0]][files[3]],\n",
    "    #conc_ts[namelist[0]][files[4]],\n",
    "    #conc_ts[namelist[0]][files[5]]\n",
    "]\n",
    "\n",
    "compare_point_concentrations_3(DataPointsConc,functionsForOverview)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concentration fluctuation analysis \n",
    "#Intermittency based on threshold, peak2Mean, concentration variance spectral density distribution\n",
    "\n",
    "#Seettings intermittency calculation\n",
    "threshold_type=\"ratio\" #ratio, absolute\n",
    "threshold_method=\"mean\" #mean, std\n",
    "intermittency_threshold=1.5 #-> if type=ratio,method mean, threshold=threshold*mean(concentration), if type=absolute: threshold=threshold\n",
    "\n",
    "for name in namelist:\n",
    "    for file in files:\n",
    "        conc_ts[name][file].analyze_concentration_fluctuations(dimensionless=\"False\",\n",
    "                                                       intermittency_threshold=intermittency_threshold,threshold_method=threshold_method)\n",
    "#power(variance) of concentration changes for different frequencies/timer interval lengths \n",
    "#Low-frequency peak: Slow, gradual concentration changes\n",
    "#High-frequency peak: Rapid, quick concentration fluctuations\n",
    "#Broad spectrum: Mixed or complex concentration dynamics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For completeness also calculate further characteristic metrics of flow/ wind velocity time series (from wtref ts), skewness ..\n",
    "#print(conc_ts[name][file].calculate_turbulence_intensity(dimensionless=\"True\",returnDistribution=\"False\",returnMetrics=\"True\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Hier kommt deine f√ºr dein Expierment und Daten angepasste Fehlerberrechnung\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
