{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Installing windtunnel package from GitHub...\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/se04ber/WTSoftwareUtilitiesShare.git\n",
      "  Cloning https://github.com/se04ber/WTSoftwareUtilitiesShare.git to /tmp/pip-req-build-di3x1rjp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/se04ber/WTSoftwareUtilitiesShare.git /tmp/pip-req-build-di3x1rjp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Resolved https://github.com/se04ber/WTSoftwareUtilitiesShare.git to commit 92ddb7445fd50ecfcd25575449ab891744d2952c\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: matplotlib in /usr/lib/python3/dist-packages (from windtunnel==0.1.0) (3.5.1)\n",
      "Requirement already satisfied: numpy in /home/sabrina/.local/lib/python3.10/site-packages (from windtunnel==0.1.0) (1.26.3)\n",
      "Requirement already satisfied: pandas in /home/sabrina/.local/lib/python3.10/site-packages (from windtunnel==0.1.0) (2.0.3)\n",
      "Requirement already satisfied: scikit-learn in /home/sabrina/.local/lib/python3.10/site-packages (from windtunnel==0.1.0) (1.4.1.post1)\n",
      "Requirement already satisfied: numpy-stl in /home/sabrina/.local/lib/python3.10/site-packages (from windtunnel==0.1.0) (3.2.0)\n",
      "Requirement already satisfied: python-utils>=3.4.5 in /home/sabrina/.local/lib/python3.10/site-packages (from numpy-stl->windtunnel==0.1.0) (3.9.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/sabrina/.local/lib/python3.10/site-packages (from pandas->windtunnel==0.1.0) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/sabrina/.local/lib/python3.10/site-packages (from pandas->windtunnel==0.1.0) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->windtunnel==0.1.0) (2022.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/sabrina/.local/lib/python3.10/site-packages (from scikit-learn->windtunnel==0.1.0) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/sabrina/.local/lib/python3.10/site-packages (from scikit-learn->windtunnel==0.1.0) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/sabrina/.local/lib/python3.10/site-packages (from scikit-learn->windtunnel==0.1.0) (1.3.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->windtunnel==0.1.0) (1.16.0)\n",
      "Requirement already satisfied: typing_extensions>3.10.0.2 in /home/sabrina/.local/lib/python3.10/site-packages (from python-utils>=3.4.5->numpy-stl->windtunnel==0.1.0) (4.9.0)\n",
      "Building wheels for collected packages: windtunnel\n",
      "  Building wheel for windtunnel (setup.py): started\n",
      "  Building wheel for windtunnel (setup.py): finished with status 'done'\n",
      "  Created wheel for windtunnel: filename=windtunnel-0.1.0-py3-none-any.whl size=126874 sha256=7053ed4766fc9f0ac00ad8676db67b3512362c91dbb0a202a713e583dbd4885e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-g1blv97a/wheels/89/af/4f/2e3531ce85996e2a3ee49d5f41b4c0c23f3ecf077df13761cc\n",
      "Successfully built windtunnel\n",
      "Installing collected packages: windtunnel\n",
      "Successfully installed windtunnel-0.1.0\n",
      "‚úÖ windtunnel package installed successfully!\n",
      "‚úÖ windtunnel package verified and ready to use!\n"
     ]
    }
   ],
   "source": [
    "# Windtunnel Package Setup\n",
    "from deploy_config import install_windtunnel, verify_installation\n",
    "\n",
    "# Install and verify windtunnel package\n",
    "if not install_windtunnel():\n",
    "    print(\"‚ùå Installation failed. Please check your internet connection.\")\n",
    "else:\n",
    "    verify_installation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Setting up folder structure...\n",
      "‚úÖ Created directory: /home/sabrina/Desktop/Schreibtisch/Arbeit_2025/SafetyCopy/WTSoftwareUtilitiesShare/Data\n",
      "‚úÖ Created directory: /home/sabrina/Desktop/Schreibtisch/Arbeit_2025/SafetyCopy/WTSoftwareUtilitiesShare/Data/InputData\n",
      "‚úÖ Created directory: /home/sabrina/Desktop/Schreibtisch/Arbeit_2025/SafetyCopy/WTSoftwareUtilitiesShare/Data/ParameterFiles\n",
      "‚úÖ Created directory: /home/sabrina/Desktop/Schreibtisch/Arbeit_2025/SafetyCopy/WTSoftwareUtilitiesShare/Results\n",
      "üåê Using GitHub example data\n",
      "üì• Downloading example data from GitHub...\n",
      "‚úÖ Already exists: UBA_GA_02_04_01_000_1_001.txt.ts#0\n",
      "‚úÖ Already exists: UBA_GA_02_04_01_000_1_001.txt.ts#1\n",
      "‚úÖ Already exists: UBA_GA_02_04_01_000_1_001.txt.ts#2\n",
      "‚úÖ Already exists: UBA_GA_02_04_01_000_1_001.txt.ts#3\n",
      "‚úÖ Already exists: UBA_GA_02_04_01_000_1_001.txt.ts#4\n",
      "‚úÖ Already exists: UBA_GA_02_04_01_000_1_001.txt.ts#5\n",
      "‚úÖ Already exists: ambient_conditions_.UBA_GA.csv\n",
      "‚úÖ Example data setup complete!\n",
      "‚úÖ Setup complete! Data path: /home/sabrina/Desktop/Schreibtisch/Arbeit_2025/SafetyCopy/WTSoftwareUtilitiesShare/Data/InputData/Beispiel Umrechnung zur Kontrolle, Output: /home/sabrina/Desktop/Schreibtisch/Arbeit_2025/SafetyCopy/WTSoftwareUtilitiesShare/Results/\n"
     ]
    }
   ],
   "source": [
    "# Data Setup and Configuration\n",
    "from deploy_config import setup_folder_structure, setup_github_data, setup_local_data\n",
    "\n",
    "# Setup folder structure and data\n",
    "base_dir, data_dir, input_dir, param_dir, results_dir = setup_folder_structure()\n",
    "\n",
    "# Data source configuration\n",
    "USE_GITHUB_EXAMPLE_DATA = True  # Set to False to use local data\n",
    "\n",
    "if USE_GITHUB_EXAMPLE_DATA:\n",
    "    path_dir, path, csv_file, output_path, namelist = setup_github_data(input_dir, param_dir, results_dir)\n",
    "else:\n",
    "    # Local data configuration\n",
    "    DATA_FOLDER_NAME = \"your_data_folder\"\n",
    "    PARAMETER_FILE_NAME = \"ambient_conditions.csv\" \n",
    "    MEASUREMENT_PREFIX = \"your_measurement_prefix\"\n",
    "    path_dir, path, csv_file, output_path, namelist = setup_local_data(\n",
    "        input_dir, param_dir, results_dir, DATA_FOLDER_NAME, PARAMETER_FILE_NAME, MEASUREMENT_PREFIX\n",
    "    )\n",
    "\n",
    "print(f\"‚úÖ Setup complete! Data path: {path}, Output: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Analysis parameters configured!\n"
     ]
    }
   ],
   "source": [
    "# Analysis Parameters\n",
    "# Measurement configuration\n",
    "parameters_PerFolder = False\n",
    "\n",
    "# Variables and Parameters set for all ts, if no ambient_conditions.csv file overgiven\n",
    "# If at the end calculate entdimensionalised or full scale transform quantities\n",
    "# Default: nd:entdimensionalise, ms:model scale, fs:full scale.    \n",
    "full_scale='ms'\n",
    "# Postprocessing before analysis\n",
    "applyPostprocessing=True\n",
    "averageInterval=60 # s  # Interval to downaverage raw time series to before analysis\n",
    "measurementFreq=0.005 # Time series frequency # For now only for static case implemented\n",
    "averagingColumns=[\"net_concentration\"] # Columns to average dow\n",
    "# Saving settings: (output_path for path)\n",
    "osType = \"Linux\" # Windows  # For Path\n",
    "outputName = None # Default: ts name\n",
    "\n",
    "saveTs=True    # Only save time series of concentration quantities to separat files\n",
    "saveAvg=True # Save average of ts of concentration quantities to separat files\n",
    "saveStats=True # Save averages, percentile95/5, peak2mean of ts of concentration quantities to separat files\n",
    "saveCombined=True # Save all averages and statistics for all files to one combined file\n",
    "combinedFileName=\"combined_file_nora.csv\"\n",
    "base_path=None # base_path = output_path + \"Files/Point_Data_stats/UBA_thesi/\" # Base path for getting files for combined files, if None\n",
    "\n",
    "saveAll=True # Sets saveTs, saveAvg and saveStats, saveCombined to True, saving ts, averages, statistics and combined file\n",
    "\n",
    "# Uncertainty calculation and saving\n",
    "calculateUncertainty=True # Calculate measurement uncertainties based on repeated measurements\n",
    "saveUncertainties=True # Add uncertainty columns to saved files\n",
    "saveConfigNames=True # Add config_name column to combined files\n",
    "split_factor=1.8 # Factor for splitting long time series in uncertainty calculation\n",
    "uncertainty_threshold=1e-4 # Threshold for relative deviation calculation\n",
    "verboseUncertainty=True # Print detailed logging for uncertainty calculation\n",
    "uncertaintyMetrics=None # Metrics to calculate uncertainties for: [\"Mean\", \"Median\", \"Peak2Mean\", \"P95\"] or None for all\n",
    "uncertaintyConcentrationTypes=None # Concentration types: [\"c_star\", \"net_concentration\", \"full_scale_concentration\"] or None for c_star only\n",
    "includeAbsoluteUncertainty=True # Include absolute uncertainty values (_uncertainty_abs)\n",
    "includePercentageUncertainty=True # Include percentage uncertainty values (_uncertainty_pct)\n",
    "\n",
    "# Column selection for combined files\n",
    "columnsToSave=None # None=default columns, or provide list e.g. [\"Avg_c_star [-]\", \"X_fs [m]\"]\n",
    "\n",
    "# Legacy uncertainty (for plotting)\n",
    "uncertainty_value=None # Manual uncertainty value for error bars\n",
    "uncertainty_representation=\"percentage\" # \"absoluteValue\"\n",
    "\n",
    "print(\"‚úÖ Analysis parameters configured!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example file/Default environment values if no csv_file found:\n",
    "\n",
    "#Source location  [mm]\n",
    "x_source=0\n",
    "y_source=0\n",
    "z_source=0\n",
    "#Source mass flow controller, calibration settings\n",
    "mass_flow_controller=0.300 #0.600#Stickstoffdurchflussregler #[l/h]*1/100 #'X'  #Controller(settings) used, just a name placeholder for orientation, not used yet\n",
    "#If calibration performed on a controller, corrects actual max. flow capacity of controller\n",
    "calibration_curve=1.0     #0.3     #0.3 oder 3\n",
    "calibration_factor=0 #1      #\n",
    "#Gas characteristics\n",
    "gas_name='C12'           #Just placeholder name variable for orientation, not used for anything\n",
    "gas_factor=0.5   #[-]    #!!! Needs to be calculate/specificate f.e. if gas changes \n",
    "mol_weight=29.0 #28.97 #Air [g/mol]\n",
    "#Measurement location [mm]\n",
    "x_measure=1020 #855.16\n",
    "y_measure= 0    #176.29\n",
    "z_measure= 5     #162\n",
    "#Surrounding conditions\n",
    "pressure=101426.04472        #1009.38  #[hPa] ->Pa\n",
    "temperature=23             #23.5  #[¬∞C]\n",
    "#Model to Reality scaling\n",
    "scale=400                     #250      #Model/Reality\n",
    "scaling_factor=0.5614882               #0.637       #USA1 to selected ref pos.?\n",
    "ref_length=1/400              #1/250           #Lref\n",
    "ref_height=100/400            #None            #Href\n",
    "#Full Scale Parameters\n",
    "full_scale_wtref=10             #6         #Uref_fullscale\n",
    "full_scale_flow_rate=0.002     #Q_amb[kg/s]?   #0.5   #Qv_fullscale\n",
    "full_scale_temp=20             #[¬∞C]\n",
    "full_scale_pressure=101325     #[Pa]\n",
    "#Q_ambient[kg/s] ->  Q[m¬≥/s]=Q[kg/s]*R*T/(M*p)\n",
    "#Variable wdir for wind direction. To be implemented in future. ##\n",
    "#wdir=0\n",
    "#Variable axis_range. Reserved for future implementation of axis range specification, \n",
    "#analogously to puff mode\n",
    "#axis_range='auto'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV erwartet unter: /home/sabrina/Desktop/Schreibtisch/Arbeit_2025/SafetyCopy/WTSoftwareUtilitiesShare/Data/ParameterFiles/ambient_conditions_.UBA_GA.csv\n",
      "‚úÖ CSV-Datei gefunden.\n",
      "files: ['UBA_GA_02_04_01_000_1_001.txt.ts#0', 'UBA_GA_02_04_01_000_1_001.txt.ts#1', 'UBA_GA_02_04_01_000_1_001.txt.ts#2', 'UBA_GA_02_04_01_000_1_001.txt.ts#3', 'UBA_GA_02_04_01_000_1_001.txt.ts#4', 'UBA_GA_02_04_01_000_1_001.txt.ts#5']\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "/home/sabrina/Desktop/Schreibtisch/Arbeit_2025/SafetyCopy/WTSoftwareUtilitiesShare/Data/InputData/Beispiel Umrechnung zur KontrolleUBA_GA_02_04_01_000_1_001.txt.ts#0 not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 85\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m#Read ambient conditions from csv file\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m         x_source, y_source, z_source, x_measure, y_measure, z_measure, pressure, temperature, calibration_curve, mass_flow_controller, calibration_factor, scaling_factor, scale, ref_length, \\\n\u001b[1;32m     82\u001b[0m         ref_height, gas_name, mol_weight, gas_factor, full_scale_wtref, full_scale_flow_rate, full_scale_temp, full_scale_pressure,  config_name\u001b[38;5;241m=\u001b[39m wt\u001b[38;5;241m.\u001b[39mPointConcentration\u001b[38;5;241m.\u001b[39mread_ambient_conditions(\n\u001b[1;32m     83\u001b[0m         ambient_conditions, file)\n\u001b[0;32m---> 85\u001b[0m conc_ts[name][file] \u001b[38;5;241m=\u001b[39m \u001b[43mwt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPointConcentration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m conc_ts[name][file]\u001b[38;5;241m.\u001b[39mambient_conditions(x_source\u001b[38;5;241m=\u001b[39mx_source, y_source\u001b[38;5;241m=\u001b[39my_source, z_source\u001b[38;5;241m=\u001b[39mz_source,\n\u001b[1;32m     88\u001b[0m                                        x_measure\u001b[38;5;241m=\u001b[39mx_measure, y_measure\u001b[38;5;241m=\u001b[39my_measure, z_measure\u001b[38;5;241m=\u001b[39mz_measure,\n\u001b[1;32m     89\u001b[0m                                        pressure\u001b[38;5;241m=\u001b[39mpressure,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m                                        calibration_factor\u001b[38;5;241m=\u001b[39mcalibration_factor,\n\u001b[1;32m     94\u001b[0m                                        config_name\u001b[38;5;241m=\u001b[39mconfig_name)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m#Set read-in scaling, gas and full scale information to internal class variables\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Schreibtisch/Arbeit_2025/SafetyCopy/WTSoftwareUtilitiesShare/windtunnel/concentration/PointConcentration.py:124\u001b[0m, in \u001b[0;36mPointConcentration.from_file\u001b[0;34m(cls, filename)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Create PointConcentration object from file. open_rate is converted\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03mto %.\"\"\"\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m#TODO: open rate = flow rate?\t\t\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m time, wtref, slow_FID, fast_FID, open_rate \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenfromtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43munpack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(time, wtref, slow_FID, fast_FID, open_rate)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/lib/npyio.py:1980\u001b[0m, in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, ndmin, like)\u001b[0m\n\u001b[1;32m   1978\u001b[0m     fname \u001b[38;5;241m=\u001b[39m os_fspath(fname)\n\u001b[1;32m   1979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m-> 1980\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_datasource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1981\u001b[0m     fid_ctx \u001b[38;5;241m=\u001b[39m contextlib\u001b[38;5;241m.\u001b[39mclosing(fid)\n\u001b[1;32m   1982\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/lib/_datasource.py:193\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03mOpen `path` with `mode` and return the file object.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m \n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    192\u001b[0m ds \u001b[38;5;241m=\u001b[39m DataSource(destpath)\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/lib/_datasource.py:533\u001b[0m, in \u001b[0;36mDataSource.open\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _file_openers[ext](found, mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m    531\u001b[0m                               encoding\u001b[38;5;241m=\u001b[39mencoding, newline\u001b[38;5;241m=\u001b[39mnewline)\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 533\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: /home/sabrina/Desktop/Schreibtisch/Arbeit_2025/SafetyCopy/WTSoftwareUtilitiesShare/Data/InputData/Beispiel Umrechnung zur KontrolleUBA_GA_02_04_01_000_1_001.txt.ts#0 not found."
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import windtunnel as wt\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import importlib\n",
    "import warnings\n",
    "\n",
    "# Suppress pandas warnings about attribute access (these are just warnings, not errors)\n",
    "warnings.filterwarnings(\"ignore\", message=\"Pandas doesn't allow columns to be created via a new attribute name\")\n",
    "\n",
    "from windtunnel.concentration.CompareDatasets import *\n",
    "\n",
    "# Reload utils module to get latest changes\n",
    "if 'windtunnel.concentration.utils' in sys.modules:\n",
    "    importlib.reload(sys.modules['windtunnel.concentration.utils'])\n",
    "\n",
    "#Edited Nora: Check if ambient conditions file is even there, before trying out reading-in\n",
    "print(f\"CSV erwartet unter: {csv_file}\")\n",
    "if os.path.exists(csv_file):\n",
    "    print(\"‚úÖ CSV-Datei gefunden.\")\n",
    "else:\n",
    "    print(\"‚ùå CSV-Datei NICHT gefunden.\")\n",
    "    print(\"Folgende Dateien sind im Ordner vorhanden:\")\n",
    "    for f in os.listdir(path):\n",
    "        print(f)\n",
    "\n",
    "#Initialize uncertainty results\n",
    "uncertainty_results = {}\n",
    "        \n",
    "###### Initialise concentration ts dictionary of length namelist, as well as for full scale and entdimensionalised\n",
    "conc_ts = {}\n",
    "conc_ts.fromkeys(namelist)\n",
    "conc_ts_fs = conc_ts\n",
    "conc_ts_nd = conc_ts\n",
    "\n",
    "dict_conc_ts = conc_ts\n",
    "dict_conc_nd = conc_ts\n",
    "dict_conc_fs = conc_ts\n",
    "\n",
    "data_dict = {}\n",
    "data_dict.fromkeys(namelist)\n",
    "\n",
    "#Read in ambient conditions for each folder or concentration ts from given csv file or for same conditions from manually\n",
    "parameters_PerFolder = parameters_PerFolder #False  #True=for each folder/namelist entry new column, False: for each ts one column entry\n",
    "\n",
    "for name in namelist:\n",
    "    files = wt.get_files(path, name)\n",
    "    print(f\"files: {files}\")\n",
    "\n",
    "    #Initilise Dictionary for each given name containing dimensions of nr of files ts#0-\n",
    "    conc_ts[name] = {}\n",
    "    conc_ts[name].fromkeys(files)\n",
    "    \n",
    "    if parameters_PerFolder==True:\n",
    "        #Read ambient conditions from csv file only for each folder\n",
    "        ambient_conditions = wt.PointConcentration.get_ambient_conditions(path=path, name=name, input_file=csv_file)\n",
    "        #print(ambient_conditions)\n",
    "        #Else read/use given default from cell above\n",
    "        if ambient_conditions is None:\n",
    "            []\n",
    "        #Read ambient conditions from csv file\n",
    "        else:\n",
    "            x_source, y_source, z_source, x_measure, y_measure, z_measure, pressure, temperature, calibration_curve, mass_flow_controller, calibration_factor, scaling_factor, scale, ref_length, \\\n",
    "            ref_height, gas_name, mol_weight, gas_factor, full_scale_wtref, full_scale_flow_rate, full_scale_temp, full_scale_pressure, config_name = wt.PointConcentration.read_ambient_conditions(\n",
    "                ambient_conditions, name)\n",
    "            \n",
    "        \n",
    "    for file in files:\n",
    "        if parameters_PerFolder == False:\n",
    "            #Read in ambient condition column for each ts\n",
    "            ambient_conditions = wt.PointConcentration.get_ambient_conditions(path=path, name=file, input_file=csv_file)\n",
    "            #Else read/use given default from cell above\n",
    "            if ambient_conditions is None:\n",
    "                []\n",
    "            #Read ambient conditions from csv file\n",
    "            else:\n",
    "                x_source, y_source, z_source, x_measure, y_measure, z_measure, pressure, temperature, calibration_curve, mass_flow_controller, calibration_factor, scaling_factor, scale, ref_length, \\\n",
    "                ref_height, gas_name, mol_weight, gas_factor, full_scale_wtref, full_scale_flow_rate, full_scale_temp, full_scale_pressure,  config_name= wt.PointConcentration.read_ambient_conditions(\n",
    "                ambient_conditions, file)\n",
    "\n",
    "        conc_ts[name][file] = wt.PointConcentration.from_file(path + file)\n",
    "    \n",
    "        conc_ts[name][file].ambient_conditions(x_source=x_source, y_source=y_source, z_source=z_source,\n",
    "                                               x_measure=x_measure, y_measure=y_measure, z_measure=z_measure,\n",
    "                                               pressure=pressure,\n",
    "                                               temperature=temperature,\n",
    "                                               calibration_curve=calibration_curve,\n",
    "                                               mass_flow_controller=mass_flow_controller,\n",
    "                                               calibration_factor=calibration_factor,\n",
    "                                               config_name=config_name)\n",
    "\n",
    "        #Set read-in scaling, gas and full scale information to internal class variables\n",
    "        print(\"Store information into PointConcentration class objects array\")\n",
    "        conc_ts[name][file].scaling_information(scaling_factor=scaling_factor, \n",
    "                                                scale=scale,\n",
    "                                                ref_length=ref_length, \n",
    "                                                ref_height=ref_height)\n",
    "        conc_ts[name][file].tracer_information(gas_name=gas_name,\n",
    "                                               mol_weight=mol_weight,\n",
    "                                               gas_factor=gas_factor)\n",
    "        conc_ts[name][file].full_scale_information(full_scale_wtref=full_scale_wtref,\n",
    "                                                   full_scale_flow_rate=full_scale_flow_rate,\n",
    "                                                   full_scale_temp=full_scale_temp,full_scale_pressure=full_scale_pressure)\n",
    "\n",
    "        #Calculate mass flow rate, net concentration and dimensionalise concentration\n",
    "        print(\"Do main calculations\")\n",
    "        conc_ts[name][file].convert_temperature()\n",
    "        conc_ts[name][file].calc_wtref_mean()\n",
    "        \n",
    "        conc_ts[name][file].calc_model_mass_flow_rate(usingMaxFlowRate=\"True\",applyCalibration=\"False\")\n",
    "        conc_ts[name][file].calc_net_concentration()\n",
    "\n",
    "        #conc_ts[name][file].clear_zeros()  #Remove values net_concentration =< 0 from dataset !noise\n",
    "        conc_ts[name][file].calc_c_star()\n",
    "\n",
    "        conc_ts[name][file].calc_full_scale_concentration() #Try\n",
    "\n",
    "        #Transforming/Outputting data in full-scale, model scale, and non-dimensional\n",
    "        print(\"Transform scale\")\n",
    "        if full_scale == 'ms':\n",
    "            dict_conc_ts = conc_ts\n",
    "            \n",
    "        elif full_scale == 'fs':\n",
    "            dict_conc_ts = conc_ts_fs\n",
    "            dict_conc_ts[name][file].to_full_scale()\n",
    "            \n",
    "        elif full_scale == 'nd':\n",
    "            dict_conc_ts = conc_ts_nd\n",
    "            dict_conc_ts[name][file].to_non_dimensional()\n",
    "        else:\n",
    "            print(\n",
    "                \"Error: invalid input for full_scale. Data can only be computed in model scale (full_scale='ms'), full scale (full_scale='fs'), or non-dimensionally (full_scale=nd).\")\n",
    "        #Apply Postprocessing if overgiven\n",
    "        \"\"\"\n",
    "        measurementFreq=measurementFreq #Time series frequency #For now only for static case implemented\n",
    "        averageInterval=averageInterval #60 #s\n",
    "        columns=averagingColumns #Columns to average down\n",
    "        #print(len(dict_conc_ts[name][file].net_concentration))\n",
    "        if(applyPostprocessing==True):\n",
    "                print(\"Apply postprocessing averaging: {averageInterval}s\")\n",
    "                dict_conc_ts[name][file].downAverage(averageInterval=averageInterval,measurementFreq=measurementFreq, columns=columns)\n",
    "                #dict_conc_ts[name][file].net_concentration\n",
    "        \n",
    "        #print(len(dict_conc_ts[name][file].net_concentration))\n",
    "        \"\"\"\n",
    "    \n",
    "    #Calculate measurement uncertainties before saving combined files\n",
    "    if calculateUncertainty and saveCombined:\n",
    "        try:\n",
    "            from windtunnel.concentration.utils import calculate_uncertainties\n",
    "            if verboseUncertainty:\n",
    "                metrics_str = \", \".join(uncertaintyMetrics) if uncertaintyMetrics else \"all\"\n",
    "                conc_types_str = \", \".join(uncertaintyConcentrationTypes) if uncertaintyConcentrationTypes else \"c_star only\"\n",
    "                print(f\"\\nüìä Calculating measurement uncertainties for: {metrics_str} ({conc_types_str})\")\n",
    "            for name in namelist:\n",
    "                try:\n",
    "                    uncertainty_results.update(calculate_uncertainties(\n",
    "                        conc_ts[name], \n",
    "                        split_factor=split_factor, \n",
    "                        uncertainty_threshold=uncertainty_threshold, \n",
    "                        verbose=verboseUncertainty,\n",
    "                        metrics_to_calculate=uncertaintyMetrics,\n",
    "                        concentration_types=uncertaintyConcentrationTypes,\n",
    "                        include_abs=includeAbsoluteUncertainty,\n",
    "                        include_pct=includePercentageUncertainty\n",
    "                    ))\n",
    "                except TypeError as e:\n",
    "                    print(f\"‚ö†Ô∏è Error with new parameters: {e}\")\n",
    "                    print(\"‚ö†Ô∏è Falling back to basic parameters\")\n",
    "                    uncertainty_results.update(calculate_uncertainties(conc_ts[name], split_factor, uncertainty_threshold))\n",
    "        except ImportError:\n",
    "            print(\"‚ö†Ô∏è calculate_uncertainties function not available in this version\")\n",
    "            print(\"‚ö†Ô∏è Skipping uncertainty calculation - continuing without uncertainties\")\n",
    "            uncertainty_results = None\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error calculating uncertainties: {e}\")\n",
    "            print(\"‚ö†Ô∏è Continuing without uncertainties\")\n",
    "            uncertainty_results = None\n",
    "    \n",
    "    for name in namelist:\n",
    "        #Saving PointConcObject calculates new quantities(f.e. c*star) to files\n",
    "        if(saveAll):\n",
    "            saveTs=True\n",
    "            saveAvg=True\n",
    "            saveStats=True\n",
    "            saveCombined=True\n",
    "        if(saveCombined):\n",
    "            saveAvg=True\n",
    "            saveStats=True\n",
    "                    \n",
    "        if(saveTs==True or saveAvg==True or saveStats==True or saveCombined==True):\n",
    "            if osType==\"Windows\":\n",
    "                folder = 'Point_Data\\\\' + name[:name.find('.')] + '\\\\'\n",
    "                folder_avg = 'Point_Data_avg\\\\' + name[:name.find('.')] + '\\\\'\n",
    "                folder_stats = 'Point_Data_stats\\\\' + name[:name.find('.')] + '\\\\'\n",
    "            elif osType==\"Linux\":\n",
    "                 #print(\"gets here 2\")\n",
    "                 folder = 'Point_Data/' + name[:name.find('.')] + '/'\n",
    "                 folder_avg = 'Point_Data_avg/' + name[:name.find('.')] + '/'\n",
    "                 folder_stats = 'Point_Data_stats/' + name[:name.find('.')] + '/'\n",
    "              \n",
    "            wt.check_directory(output_path + folder)\n",
    "            dict_conc_ts[next(iter(conc_ts))][next(iter(conc_ts[next(iter(conc_ts))]))].__check_sum = 8\n",
    "        #dict_conc_ts[name][file].__check_sum = 8\n",
    "      \n",
    "        if(saveTs):\n",
    "            if full_scale == 'ms':\n",
    "                dict_conc_ts[name][file].save2file_ms(file, out_dir=output_path + folder)\n",
    "            elif full_scale == 'fs':\n",
    "                dict_conc_ts[name][file].save2file_fs(file, out_dir=output_path + folder)\n",
    "            elif full_scale == 'nd':\n",
    "                dict_conc_ts[name][file].save2file_nd(file, out_dir=output_path + folder)\n",
    "            else:\n",
    "                print(\n",
    "                    \"Error: invalid input for full_scale. Data can only be computed in model scale (full_scale='ms'), full scale (full_scale='fs'), or non-dimensionally (full_scale=nd).\")\n",
    "                exit\n",
    "            print(\"Created ts files including (net_concentration, entimendionsliased and full scale concentration)\")\n",
    "        if(saveAvg):\n",
    "             #Saving averages to files under folder Point_Data_stats/\n",
    "            #Averages of net_concentration,c_star and full_scale_concentration\n",
    "             wt.check_directory(output_path + folder_avg)\n",
    "             dict_conc_ts[name][file].save2file_avg(file, out_dir=output_path + folder_avg)\n",
    "             print(f\"Created avg files under {output_path + folder_avg}\")\n",
    "        if(saveStats):\n",
    "            #Saving stats to files under folder Point_Data_avg/\n",
    "            #Stats Full ausgabe: saveAvg Quantities + Percentile 95, percentile 5, peak2Mean of net_concentration_c_star and full_Scale_conc\n",
    "             wt.check_directory(output_path + folder_stats)\n",
    "             dict_conc_ts[name][file].save2file_fullStats(file, out_dir=output_path + folder_stats)\n",
    "             print(f\"Created stats files under {output_path + folder_stats}\")\n",
    "            \n",
    "        if(saveCombined):\n",
    "            try:\n",
    "                from windtunnel.concentration.utils import combine_to_csv2\n",
    "                stats=True\n",
    "                if(stats):\n",
    "                    file_type=\"stats\"\n",
    "                \n",
    "                file_names = [\"_stats_\" + file for file in files]\n",
    "                if(base_path==None):\n",
    "                    base_path = output_path + f\"Point_Data_{file_type}/{name[0:-1]}/\"\n",
    "                \n",
    "                combine_to_csv2(\n",
    "                    file_names=file_names,\n",
    "                    base_path=base_path,\n",
    "                    file_type=file_type,\n",
    "                    output_filename=output_path+combinedFileName,\n",
    "                    conc_ts_dict=conc_ts[name],\n",
    "                    uncertainty_results=uncertainty_results if calculateUncertainty else None,\n",
    "                    save_config_names=saveConfigNames,\n",
    "                    save_uncertainties=saveUncertainties and calculateUncertainty,\n",
    "                    columns_to_save=columnsToSave\n",
    "                )\n",
    "                print(f\"‚úÖ Created combined file: {output_path+combinedFileName}\")\n",
    "            except ImportError:\n",
    "                # Fallback to basic combine_to_csv function\n",
    "                try:\n",
    "                    from windtunnel.concentration.utils import combine_to_csv\n",
    "                    stats=True\n",
    "                    if(stats):\n",
    "                        file_type=\"stats\"\n",
    "                    \n",
    "                    # The saved files have \"_stats_\" prefix, so we need to use that\n",
    "                    file_names = [\"_stats_\" + file for file in files]\n",
    "                    if(base_path==None):\n",
    "                        base_path = output_path + f\"Point_Data_{file_type}/{name[0:-1]}/\"\n",
    "                    \n",
    "                    combine_to_csv(\n",
    "                        file_names=file_names,\n",
    "                        base_path=base_path,\n",
    "                        file_type=file_type,\n",
    "                        output_filename=output_path+combinedFileName\n",
    "                    )\n",
    "                    print(f\"‚úÖ Created combined file (basic version): {output_path+combinedFileName}\")\n",
    "                except ImportError:\n",
    "                    print(\"‚ö†Ô∏è No combine functions available in this version\")\n",
    "                    print(\"‚ö†Ô∏è Skipping combined file creation - individual files saved successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error creating combined file: {e}\")\n",
    "                print(\"‚ö†Ô∏è Individual files saved successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is no longer needed - combined file creation is now in the main processing cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncertainty calculation is now integrated in Cell 2 and modularized in utils.calculate_uncertainties()\n",
    "#This cell is no longer needed - uncertainty calculation happens automatically before saveCombined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test read-in and net_conc and c*star output values, shape, nans and min/max\n",
    "#Test if net_concentration and c_star there\n",
    "\"\"\"if hasattr(conc_ts[name][file], \"c_star\"):\n",
    "    print(\"c_star vorhanden\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Keine c_star Daten gefunden.\")\n",
    "\n",
    "if hasattr(conc_ts[name][file], \"concentration\"):\n",
    "    if hasattr(conc_ts[name][file], \"net_concentration\"):\n",
    "        setattr(conc_ts[name][file], \"concentration\", conc_ts[name][file].net_concentration)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Kein concentration oder net_concentration gefunden.\")\"\"\"\n",
    "\n",
    " \n",
    "#Test numerical value range\n",
    "for name in namelist:\n",
    "    for file in files:\n",
    "        print(f\"\\n File: {file}\")\n",
    "        if hasattr(conc_ts[name][file], \"c_star\"):\n",
    "            print(\"c_star vorhanden\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Keine c_star Daten gefunden.\")\n",
    "\n",
    "        print(\"C_star shape:\", conc_ts[name][file].net_concentration.shape)\n",
    "        #print(conc_ts[name][file].net_concentration)\n",
    "        #plt.plot(conc_ts[namelist[0]][files[0]].c_star)\n",
    "        print(\"NaNs vorhanden:\", np.any(np.isnan(conc_ts[name][file].net_concentration)))\n",
    "        print(\"Min/Max:\", np.min(conc_ts[name][file].net_concentration), \"/\", np.max(conc_ts[name][file].net_concentration))\n",
    "        print(f\"Mean: {np.mean(conc_ts[name][file].net_concentration)}\")\n",
    "        print(f\"Std: {np.std(conc_ts[name][file].net_concentration)}\")\n",
    "        print(f\"Percentiles: {conc_ts[name][file].calc_percentiles(percentiles=[10, 90, 95], var='net_concentration')}\")    \n",
    "plt.show()\n",
    "#plt.violinplot([conc_ts[name][file].c_star for file in files])\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "#Read-in Data into separat array for later plotting (files/series can also be specifically choosen)\n",
    "dimensionless=True\n",
    "color=\"blue\"\n",
    "\n",
    "DataPointsConc = []\n",
    "#DataPointsConc = [ conc_ts[namelist[0]][\"MyFileofInterest1.txt\"], conc_ts[namelist[0]][\"MyFileofInterest2.txt\"]]\n",
    "for i in range(len(files)): #Just visualising all\n",
    "    data = conc_ts[namelist[0]][files[i]]\n",
    "    DataPointsConc.append(data)\n",
    "# Richtige Zeitserien laden\n",
    "labels=[f\"Dataset {i}\" for i in range(0,len(DataPointsConc))]\n",
    "plot_timeseries_with_stats(DataPointsConc,dimensionless=dimensionless,labels=labels,color=color)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#full_path = \"/Users/norakaiser/Masterthesis/WTSoftwareUtilitiesShare/test_meas/results/Point_Data_avg/UBA_thesis_test_01072025_001/_avg_UBA_thesis_test_01072025_001.ts#0\"\n",
    "full_path = \"/Users/norakaiser/Masterthesis/WTSoftwareUtilitiesShare/test_meas/results/Point_Data_avg/UBA_thesis_test_01072025_001/_avg_UBA_thesis_test_01072025_001.ts#0\"\n",
    "base_path = output_path + \"Files/Point_Data_avg/UBA_thesi/\"\n",
    "file_names = [\"_avg_\" + file for file in files]\n",
    "for file in file_names:\n",
    "    #metadata = {}\n",
    "    data = []\n",
    "    with open(full_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith('#'):\n",
    "                continue\n",
    "            if line.startswith('\"'):\n",
    "                # Kopfzeile mit Variablennamen\n",
    "                headers = line.replace('\"', '').split()\n",
    "                continue\n",
    "            # Wertezeile\n",
    "            values = list(map(float, line.split()))\n",
    "            data.append(values)\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=headers)\n",
    "    print(df.head())\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m     start \u001b[38;5;241m=\u001b[39m averaging_intervals[i]\n\u001b[1;32m     27\u001b[0m     end \u001b[38;5;241m=\u001b[39m averaging_intervals[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 28\u001b[0m     points \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mlogspace(np\u001b[38;5;241m.\u001b[39mlog10(start), np\u001b[38;5;241m.\u001b[39mlog10(end), num_points_between \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     29\u001b[0m     extended_intervals\u001b[38;5;241m.\u001b[39mextend(points[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     30\u001b[0m extended_intervals\u001b[38;5;241m.\u001b[39mappend(averaging_intervals[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#Plot Convergence plot at different averaging intervals\n",
    "# SCATTER CONVERGENCE PLOT (plot_type=\"convergence\") (Scatter plot of all means at each averaging interval)\n",
    "# BANDWIDTH CONVERGENCE PLOT (plot_type=\"bandwidth\") (Max - Min of all means at each averaging interval)\n",
    "\n",
    "# Configuration\n",
    "plot_type = \"both\" #\"bandwidth\"  # Options: \"bandwidth\", \"convergence\", \"both\"\n",
    "averaging_intervals = [60*0.01, 60*0.1, 60*0.5, 60*1.0, 60*2.0]  # in seconds\n",
    "num_points_between = 5  # Number of points to add between each interval pair for more continuity(0 = use original intervals only)\n",
    "set_xlog = True  #Set x-axsis to log\n",
    "\n",
    "xLabel = \"Time Interval\"\n",
    "yLabel = \"Windspeed [m/s]\"\n",
    "Labels = None\n",
    "dimensionless = True\n",
    "xAchse = None\n",
    "yAchse = None\n",
    "error_values = 0.1\n",
    "marker = 'o'\n",
    "time_freq = 0.010  # s\n",
    "\n",
    "\n",
    "#For generate extended intervals/points if requested\n",
    "if num_points_between > 0:\n",
    "    extended_intervals = []\n",
    "    for i in range(len(averaging_intervals) - 1):\n",
    "        start = averaging_intervals[i]\n",
    "        end = averaging_intervals[i + 1]\n",
    "        points = np.logspace(np.log10(start), np.log10(end), num_points_between + 2)\n",
    "        extended_intervals.extend(points[:-1])\n",
    "    extended_intervals.append(averaging_intervals[-1])\n",
    "    averaging_intervals_use = extended_intervals\n",
    "else:\n",
    "    averaging_intervals_use = averaging_intervals\n",
    "\n",
    "\n",
    "# BANDWIDTH CONVERGENCE PLOT \n",
    "if plot_type in [\"bandwidth\", \"both\"]:\n",
    "    bandwidths = []\n",
    "    for name in namelist:\n",
    "        for file in files:\n",
    "            ts_v_avg = conc_ts[name][file].get_averagedData(name, file, time_freq, averaging_intervals_use)\n",
    "            bandwidths.append([np.ptp(avg) for avg in ts_v_avg])\n",
    "    \n",
    "    fig1, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    for i, bandwidth in enumerate(bandwidths):\n",
    "        print(f\"Dataset {i}: {bandwidth}\")\n",
    "        ax1.plot(averaging_intervals_use, bandwidth, \"o-\", \n",
    "                 label=f\"Dataset {i}\" if Labels is None else Labels[i])\n",
    "    \n",
    "    ax1.set_xlabel(\"Averaging Intervals [s]\")\n",
    "    ax1.set_ylabel(\"Bandwidth [ppmV]\")\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, which='both', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# INTERVAL CONVERGENCE PLOT\n",
    "if plot_type in [\"convergence\", \"both\"]:\n",
    "    means_all = []\n",
    "    dataset_labels = []\n",
    "    for name in namelist:\n",
    "        for file in files:\n",
    "            ts_v_avg = conc_ts[name][file].get_averagedData(name, file, time_freq, averaging_intervals_use)\n",
    "            means_all.append(ts_v_avg)\n",
    "            dataset_labels.append(f\"{name} - {file}\")\n",
    "    n_datasets = len(means_all) \n",
    "    fig2, axes = plt.subplots(n_datasets, 1, figsize=(10, 4*n_datasets), sharex=False)\n",
    "    if n_datasets == 1: # Handle single dataset case\n",
    "        axes = [axes]\n",
    "    for i, (means, ax, label) in enumerate(zip(means_all, axes, dataset_labels)):\n",
    "        # Collect min and max values for each interval for grey background coloring\n",
    "        min_vals = []\n",
    "        max_vals = []\n",
    "        intervals_with_data = []\n",
    "        for interval, avg_data in zip(averaging_intervals_use, means):\n",
    "            if len(avg_data) > 0:\n",
    "                min_vals.append(np.min(avg_data))\n",
    "                max_vals.append(np.max(avg_data))\n",
    "                intervals_with_data.append(interval)\n",
    "        if len(intervals_with_data) > 0: #Also plot connected shaded region\n",
    "            ax.fill_between(intervals_with_data, min_vals, max_vals, \n",
    "                           color='grey', alpha=0.3, zorder=1)\n",
    "        #Satter points on top\n",
    "        for interval, avg_data in zip(averaging_intervals_use, means):\n",
    "            ax.scatter(np.ones(len(avg_data)) * interval,\n",
    "                      avg_data, \n",
    "                      s=15, c=\"navy\", marker='o', alpha=0.6, zorder=2)\n",
    "        ax.set_ylabel(yLabel)\n",
    "        ax.set_xlabel(\"Averaging Intervals [s]\")\n",
    "        ax.set_title(f'Convergence Test: {label}')\n",
    "        ax.grid(True, which='both', alpha=0.3)\n",
    "        if set_xlog:\n",
    "            ax.set_xscale('log')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4x1 Table means, std., peak2mean,percentile \n",
    "xLabels = \"Datasets\"\n",
    "yLabels = ['Mean  [ppmV]', '95th Percentile  [ppmV]', 'Std Dev.  [ppmV]','Peak/Mean  [-]'] #None #\"Concentration\"\n",
    "Labels = None\n",
    "dimensionless = False\n",
    "xAchse = None\n",
    "yAchse = None\n",
    "markers = ['o', 's', '^', 'd']\n",
    "colors = [\"green\",\"blue\",\"red\",\"purple\"]\n",
    "\n",
    "error_values = 0.5 #[error_mean,error_95,error_std,error_peak/mean]\n",
    "percentileValue = 95 #percentil for percentilCalculation 95->95%\n",
    "\n",
    "# Calculate stats\n",
    "x = range(len(DataPointsConc))\n",
    "DataPointsConc2 = [DataPoints.net_concentration for DataPoints in DataPointsConc]\n",
    "stats = [[np.mean(d),  np.percentile(d, percentileValue), np.std(d),np.max(d)/np.mean(d)] for d in DataPointsConc2]\n",
    "stats = list(zip(*stats))\n",
    "errors = error_values if isinstance(error_values, list) else [error_values] * len(DataPointsConc)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(8, 10))\n",
    "axes_flat = axes.flatten()  # Convert 2x2 array to 1D array\n",
    "\n",
    "for i, ax in enumerate(axes_flat):\n",
    "    ax.errorbar(x, stats[i], yerr=errors, fmt=markers[i], capsize=3, color=colors[i])\n",
    "    if yAchse: \n",
    "        ax.set_ylim(yAchse)\n",
    "    if xAchse: \n",
    "        ax.set_xlim(xAchse)\n",
    "    if i >= 2:  # Bottom row subplots\n",
    "        ax.set_xlabel(xLabels)\n",
    "    ax.set_ylabel(yLabels[i])\n",
    "    ax.grid(True)   \n",
    "    ax.set_title(data.config_name)\n",
    "    # ax.set_xticks(x)\n",
    "    # ax.set_xticklabels([f'Dataset {j+1}' for j in x])\n",
    "\n",
    "plt.tight_layout()\n",
    "if Labels != None:\n",
    "    plt.legend(Labels)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xLabel=\"Datasets\"\n",
    "yLabel=\"Mean concentration[ppmV]\" \n",
    "dimensionless=\"False\"\n",
    "xAchse = None \n",
    "yAchse=None #(72,79) #None \n",
    "error_values=0.5 #[0.5,0.2,0.1] #For error values overgive one number which is cast to all values, or an array if specify different errors for each measurements\n",
    "errorType=\"absolute\"\n",
    "test = create_means(DataPointsConc,error_values,dimensionless=dimensionless,labels=None,xLabel=xLabel,yLabel=yLabel,xAchse=xAchse,yAchse=yAchse)\n",
    "#plt.savefig(\"Mean_comparison.png\",test) #To save image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xLabel=\"Concentration[-]\"\n",
    "yLabel=\"Density\"\n",
    "dimensionless=\"True\"\n",
    "create_pdf(DataPointsConc,dimensionless=\"True\",labels=None,xLabel=xLabel,yLabel=yLabel,xAchse=None,yAchse=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xLabel=\"Datasets\"\n",
    "yLabel=\"Concentration [-]\"\n",
    "dimensionless=\"True\"\n",
    "create_violinplot(DataPointsConc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xLabel=\"Datasets\"\n",
    "yLabel=\"Concentration [-]\"\n",
    "dimensionless=\"True\"\n",
    "create_boxplot(DataPointsConc,dimensionless=\"True\",labels=None,xLabel=xLabel,yLabel=yLabel,xAchse=None,yAchse=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xLabel=\"Datasets\"\n",
    "yLabel=\"Concentration[ppmV]\"\n",
    "xLabel=\"Datasets\"\n",
    "yLabel=\"Concentration[ppmV]\"\n",
    "create_histogram(DataPointsConc,dimensionless=\"False\",labels=None,xLabel=xLabel,yLabel=yLabel,xAchse=None,yAchse=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yLabel=None\n",
    "xLabel=\"Concentration[-]\"\n",
    "dimensionless=\"False\"\n",
    "create_cdf(DataPointsConc,dimensionless=dimensionless,labels=None,xLabel=xLabel,yLabel=yLabel,xAchse=None,yAchse=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = powerDensityPlot(DataPointsConc,dimensionless=\"False\",plot=True,labels=None,xLabel=None,yLabel=None,xAchse=None,yAchse=None)\n",
    "plt.savefig(\"test.png\",test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from windtunnel.concentration.CompareDatasets import compare_point_concentrations_3\n",
    "#Give overview/comparison for the data, one plot including of of the plots choosen in the list \"functionsForOverview\n",
    "#Means, Pdf, Cdf, PowerDensity: Number of ts does not matter that much\n",
    "#Scatterplot: use only 2 ts\n",
    "#Histogram, Boxplot: would also recommend not to many, because of overlapping of the histograms for comparison, space for the boxplots..\n",
    "\n",
    "#functionsForOverview = [\"all\"] #defaul -> all of the available plots\n",
    "#all_plot_types = [\n",
    "#        \"Histogram\", \"Pdf\", \"Cdf\", \"Means\", \"BoxPlot\", \n",
    "#        \"QuantilPlot\", \"ScatterPlot\", \"ResidualPlot\", \"Autocorrelation\"\n",
    "#    ]\n",
    "    \n",
    "functionsForOverview = [\n",
    "    \"Histogram\",\n",
    "    \"BoxPlot\"\n",
    "    #\"\",\n",
    "    \"Pdf\",\n",
    "    \"Cdf\",\n",
    "    \"Means\",\n",
    "    \"PowerDensity\"\n",
    "        ]\n",
    "\n",
    "#Choose which concentration time series/PointObjekts to show in the overview/comparison plot\n",
    "DataPointsConc = [\n",
    "    conc_ts[namelist[0]][files[0]],#\n",
    "    conc_ts[namelist[0]][files[1]],\n",
    "    #conc_ts[namelist[0]][files[2]],\n",
    "    #conc_ts[namelist[0]][files[3]],\n",
    "    #conc_ts[namelist[0]][files[4]],\n",
    "    #conc_ts[namelist[0]][files[5]]\n",
    "]\n",
    "\n",
    "compare_point_concentrations_3(DataPointsConc,functionsForOverview)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concentration fluctuation analysis \n",
    "#Intermittency based on threshold, peak2Mean, concentration variance spectral density distribution\n",
    "\n",
    "#Seettings intermittency calculation\n",
    "threshold_type=\"ratio\" #ratio, absolute\n",
    "threshold_method=\"mean\" #mean, std\n",
    "intermittency_threshold=1.5 #-> if type=ratio,method mean, threshold=threshold*mean(concentration), if type=absolute: threshold=threshold\n",
    "\n",
    "for name in namelist:\n",
    "    for file in files:\n",
    "        conc_ts[name][file].analyze_concentration_fluctuations(dimensionless=\"False\",\n",
    "                                                       intermittency_threshold=intermittency_threshold,threshold_method=threshold_method)\n",
    "#power(variance) of concentration changes for different frequencies/timer interval lengths \n",
    "#Low-frequency peak: Slow, gradual concentration changes\n",
    "#High-frequency peak: Rapid, quick concentration fluctuations\n",
    "#Broad spectrum: Mixed or complex concentration dynamics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For completeness also calculate further characteristic metrics of flow/ wind velocity time series (from wtref ts), skewness ..\n",
    "#print(conc_ts[name][file].calculate_turbulence_intensity(dimensionless=\"True\",returnDistribution=\"False\",returnMetrics=\"True\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Hier kommt deine f√ºr dein Expierment und Daten angepasste Fehlerberrechnung\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
